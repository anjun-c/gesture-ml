{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MediaPipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands_instance = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks_from_image(image, hands_instance):\n",
    "    \"\"\"\n",
    "    Extract landmarks from the given image using MediaPipe.\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        hands_instance: An instance of MediaPipe Hands solution.\n",
    "\n",
    "    Returns:\n",
    "        A flattened list of landmarks (x, y coordinates) or None if no hands detected.\n",
    "    \"\"\"\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands_instance.process(rgb_image)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        landmarks = [lm.x for lm in hand_landmarks.landmark] + [lm.y for lm in hand_landmarks.landmark]\n",
    "        return landmarks\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images\n",
      "Processed 100 images\n",
      "Processed 200 images\n",
      "Processed 300 images\n",
      "Processed 400 images\n",
      "Processed 500 images\n",
      "Processed 600 images\n",
      "Processed 700 images\n",
      "Processed 800 images\n",
      "Processed 900 images\n",
      "Processed 1000 images\n",
      "Processed 1100 images\n",
      "Processed 1200 images\n",
      "Processed 1300 images\n",
      "Processed 1400 images\n",
      "Processed 1500 images\n",
      "Processed 1600 images\n",
      "Processed 1700 images\n",
      "Processed 1800 images\n",
      "Processed 1900 images\n",
      "Processed 2000 images\n",
      "Processed 2100 images\n",
      "Processed 2200 images\n",
      "Processed 2300 images\n",
      "Processed 2400 images\n",
      "Processed 2500 images\n",
      "Processed 2600 images\n",
      "Processed 2700 images\n",
      "Processed 2800 images\n",
      "Processed 2900 images\n",
      "Processed 3000 images\n",
      "Processed 3100 images\n",
      "Processed 3200 images\n",
      "Processed 3300 images\n",
      "Processed 3400 images\n",
      "Processed 3500 images\n",
      "Processed 3600 images\n",
      "Processed 3700 images\n",
      "Processed 3800 images\n",
      "Processed 3900 images\n",
      "Processed 4000 images\n",
      "Processed 4100 images\n",
      "Processed 4200 images\n",
      "Processed 4300 images\n",
      "Processed 4400 images\n",
      "Processed 4500 images\n",
      "Processed 4600 images\n",
      "Processed 4700 images\n",
      "Processed 4800 images\n",
      "Processed 4900 images\n",
      "Processed 5000 images\n",
      "Processed 5100 images\n",
      "Processed 5200 images\n",
      "Processed 5300 images\n",
      "Processed 5400 images\n",
      "Processed 5500 images\n",
      "Processed 5600 images\n",
      "Processed 5700 images\n",
      "Processed 5800 images\n",
      "Processed 5900 images\n",
      "Processed 6000 images\n",
      "Processed 6100 images\n",
      "Processed 6200 images\n",
      "Processed 6300 images\n",
      "Processed 6400 images\n",
      "Processed 6500 images\n",
      "Processed 6600 images\n",
      "Processed 6700 images\n",
      "Processed 6800 images\n",
      "Processed 6900 images\n",
      "Processed 7000 images\n",
      "Processed 7100 images\n",
      "Processed 7200 images\n",
      "Processed 7300 images\n",
      "Processed 7400 images\n",
      "Processed 7500 images\n",
      "Processed 7600 images\n",
      "Processed 7700 images\n",
      "Processed 7800 images\n",
      "Processed 7900 images\n",
      "Processed 8000 images\n",
      "Processed 8100 images\n",
      "Processed 8200 images\n",
      "Processed 8300 images\n",
      "Processed 8400 images\n",
      "Processed 8500 images\n",
      "Processed 8600 images\n",
      "Processed 8700 images\n",
      "Processed 8800 images\n",
      "Processed 8900 images\n",
      "Processed 9000 images\n",
      "Processed 9100 images\n",
      "Processed 9200 images\n",
      "Processed 9300 images\n",
      "Processed 9400 images\n",
      "Processed 9500 images\n",
      "Processed 9600 images\n",
      "Processed 9700 images\n",
      "Processed 9800 images\n",
      "Processed 9900 images\n",
      "Processed 10000 images\n",
      "Processed 10100 images\n",
      "Processed 10200 images\n",
      "Processed 10300 images\n",
      "Processed 10400 images\n",
      "Processed 10500 images\n",
      "Processed 10600 images\n",
      "Processed 10700 images\n",
      "Processed 10800 images\n",
      "Processed 10900 images\n",
      "Processed 11000 images\n",
      "Processed 11100 images\n",
      "Processed 11200 images\n",
      "Processed 11300 images\n",
      "Processed 11400 images\n",
      "Processed 11500 images\n",
      "Processed 11600 images\n",
      "Processed 11700 images\n",
      "Processed 11800 images\n",
      "Processed 11900 images\n",
      "Processed 12000 images\n",
      "Processed 12100 images\n",
      "Processed 12200 images\n",
      "Processed 12300 images\n",
      "Processed 12400 images\n",
      "Processed 12500 images\n",
      "Processed 12600 images\n",
      "Processed 12700 images\n",
      "Processed 12800 images\n",
      "Processed 12900 images\n",
      "Processed 13000 images\n",
      "Processed 13100 images\n",
      "Processed 13200 images\n",
      "Processed 13300 images\n",
      "Processed 13400 images\n",
      "Processed 13500 images\n",
      "Processed 13600 images\n",
      "Processed 13700 images\n",
      "Processed 13800 images\n",
      "Processed 13900 images\n",
      "Processed 14000 images\n",
      "Processed 14100 images\n",
      "Processed 14200 images\n",
      "Processed 14300 images\n",
      "Processed 14400 images\n",
      "Processed 14500 images\n",
      "Processed 14600 images\n",
      "Processed 14700 images\n",
      "Processed 14800 images\n",
      "Processed 14900 images\n",
      "Processed 15000 images\n",
      "Processed 15100 images\n",
      "Processed 15200 images\n",
      "Processed 15300 images\n",
      "Processed 15400 images\n",
      "Processed 15500 images\n",
      "Processed 15600 images\n",
      "Processed 15700 images\n",
      "Processed 15800 images\n",
      "Processed 15900 images\n",
      "Processed 16000 images\n",
      "Processed 16100 images\n",
      "Processed 16200 images\n",
      "Processed 16300 images\n",
      "Processed 16400 images\n",
      "Processed 16500 images\n",
      "Processed 16600 images\n",
      "Processed 16700 images\n",
      "Processed 16800 images\n",
      "Processed 16900 images\n",
      "Processed 17000 images\n",
      "Processed 17100 images\n",
      "Processed 17200 images\n",
      "Processed 17300 images\n",
      "Processed 17400 images\n",
      "Processed 17500 images\n",
      "Processed 17600 images\n",
      "Processed 17700 images\n",
      "Processed 17800 images\n",
      "Processed 17900 images\n",
      "Finished extracting landmarks from dataset.\n"
     ]
    }
   ],
   "source": [
    "# Convert the dataset of hand gesture images to landmark data and save it to a CSV\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the paths\n",
    "dataset_path = \"../data/archive/train\"  # Path to the existing dataset\n",
    "output_csv_path = \"landmarks_dataset.csv\"\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transforms.ToTensor())\n",
    "\n",
    "# Open CSV file to write landmarks data\n",
    "with open(output_csv_path, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Write header: landmarks (42 values) + label\n",
    "    header = [f'lm_{i}' for i in range(42)] + ['label']\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Iterate over the dataset to extract landmarks and write to CSV\n",
    "    for i, (img, label) in enumerate(dataset):\n",
    "        # Convert the PIL image to OpenCV format\n",
    "        img = transforms.ToPILImage()(img)  # Convert tensor to PIL image\n",
    "        img = np.array(img)  # Convert PIL image to NumPy array (OpenCV format)\n",
    "\n",
    "        # Extract landmarks\n",
    "        landmarks = extract_landmarks_from_image(img, hands_instance)\n",
    "\n",
    "        if landmarks is not None:\n",
    "            # Append the label to landmarks\n",
    "            row = landmarks + [label]\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i} images\")\n",
    "\n",
    "print(\"Finished extracting landmarks from dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for the landmark data\n",
    "class GestureLandmarkDataset(DataLoader):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        landmarks = row[:-1].values.astype(np.float32)\n",
    "        label = int(row[-1])\n",
    "        return torch.tensor(landmarks), torch.tensor(label)\n",
    "\n",
    "# Load the landmark dataset\n",
    "landmark_dataset = GestureLandmarkDataset(\"landmarks_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 20\n",
    "\n",
    "class GestureRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GestureRecognitionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(42, 128)  # 21 landmarks * 2 (x and y)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GestureRecognitionModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return False\n",
    "\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 validation images\n",
      "Processed 100 validation images\n",
      "Processed 200 validation images\n",
      "Processed 300 validation images\n",
      "Processed 400 validation images\n",
      "Processed 500 validation images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\anaconda3\\envs\\gesture_env\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 validation images\n",
      "Processed 700 validation images\n",
      "Processed 800 validation images\n",
      "Processed 900 validation images\n",
      "Processed 1000 validation images\n",
      "Processed 1100 validation images\n",
      "Processed 1200 validation images\n",
      "Processed 1300 validation images\n",
      "Processed 1400 validation images\n",
      "Processed 1500 validation images\n",
      "Processed 1600 validation images\n",
      "Processed 1700 validation images\n",
      "Processed 1800 validation images\n",
      "Processed 1900 validation images\n",
      "Processed 2000 validation images\n",
      "Processed 2100 validation images\n",
      "Processed 2200 validation images\n",
      "Processed 2300 validation images\n",
      "Processed 2400 validation images\n",
      "Processed 2500 validation images\n",
      "Processed 2600 validation images\n",
      "Processed 2700 validation images\n",
      "Processed 2800 validation images\n",
      "Processed 2900 validation images\n",
      "Processed 3000 validation images\n",
      "Processed 3100 validation images\n",
      "Processed 3200 validation images\n",
      "Processed 3300 validation images\n",
      "Processed 3400 validation images\n",
      "Processed 3500 validation images\n",
      "Processed 3600 validation images\n",
      "Processed 3700 validation images\n",
      "Processed 3800 validation images\n",
      "Processed 3900 validation images\n",
      "Processed 4000 validation images\n",
      "Processed 4100 validation images\n",
      "Processed 4200 validation images\n",
      "Processed 4300 validation images\n",
      "Processed 4400 validation images\n",
      "Processed 4500 validation images\n",
      "Processed 4600 validation images\n",
      "Processed 4700 validation images\n",
      "Processed 4800 validation images\n",
      "Processed 4900 validation images\n",
      "Processed 5000 validation images\n",
      "Processed 5100 validation images\n",
      "Processed 5200 validation images\n",
      "Processed 5300 validation images\n",
      "Processed 5400 validation images\n",
      "Processed 5500 validation images\n",
      "Processed 5600 validation images\n",
      "Processed 5700 validation images\n",
      "Processed 5800 validation images\n",
      "Processed 5900 validation images\n",
      "Finished extracting landmarks from validation dataset.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "val_dataset_path = \"../data/archive/test\"\n",
    "val_dataset = datasets.ImageFolder(root=val_dataset_path, transform=transforms.ToTensor())\n",
    "\n",
    "# Extract landmarks for validation dataset and save to CSV\n",
    "val_output_csv_path = \"landmarks_val_dataset.csv\"\n",
    "with open(val_output_csv_path, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    header = [f'lm_{i}' for i in range(42)] + ['label']\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for i, (img, label) in enumerate(val_dataset):\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        img = np.array(img)\n",
    "        landmarks = extract_landmarks_from_image(img, hands_instance)\n",
    "\n",
    "        if landmarks is not None:\n",
    "            row = landmarks + [label]\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i} validation images\")\n",
    "\n",
    "print(\"Finished extracting landmarks from validation dataset.\")\n",
    "\n",
    "# Load validation landmark dataset\n",
    "val_landmark_dataset = GestureLandmarkDataset(\"landmarks_val_dataset.csv\")\n",
    "\n",
    "train_loader = DataLoader(landmark_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_landmark_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def train_gesture_model(epochs=10, patience=4):\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for data, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping(val_loss):\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 3.1159\n",
      "Validation Loss: 3.0546\n",
      "Epoch [2/10], Training Loss: 3.1051\n",
      "Validation Loss: 3.0238\n",
      "Epoch [3/10], Training Loss: 2.9896\n",
      "Validation Loss: 2.9941\n",
      "Epoch [4/10], Training Loss: 3.0896\n",
      "Validation Loss: 2.9674\n",
      "Epoch [5/10], Training Loss: 2.9735\n",
      "Validation Loss: 2.9391\n",
      "Epoch [6/10], Training Loss: 2.9408\n",
      "Validation Loss: 2.9102\n",
      "Epoch [7/10], Training Loss: 2.9349\n",
      "Validation Loss: 2.8804\n",
      "Epoch [8/10], Training Loss: 2.9605\n",
      "Validation Loss: 2.8488\n",
      "Epoch [9/10], Training Loss: 2.8625\n",
      "Validation Loss: 2.8162\n",
      "Epoch [10/10], Training Loss: 2.9076\n",
      "Validation Loss: 2.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_30664\\57345037.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label = int(row[-1])\n"
     ]
    }
   ],
   "source": [
    "train_gesture_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesture recognition function (for real-time prediction)\n",
    "def recognize_gesture(landmarks):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        landmarks = torch.tensor(landmarks, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        output = model(landmarks)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use in the main detection loop\n",
    "def gesture_recognition_integration(hand_landmarks):\n",
    "    if hand_landmarks:\n",
    "        landmarks_array = np.array([[lm.x, lm.y] for lm in hand_landmarks.landmark]).flatten()\n",
    "        predicted_gesture = recognize_gesture(landmarks_array)\n",
    "        return predicted_gesture\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
