{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from capture import initialize_webcam, display_frame, capture_video, release_resources\n",
    "from gesture_rec import gesture_recognition_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init mediapipe hands & drawing\n",
    "mp_hands_solution = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands_instance = mp_hands_solution.Hands(max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_detection(frame, hands):\n",
    "    \"\"\"\n",
    "    Perform hand detection using MediaPipe and return the processed frame with landmarks.\n",
    "    \"\"\"\n",
    "    # convert the image to RGB - mediapipe expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # process frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # draw landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Use the HAND_CONNECTIONS directly from mp.solutions.hands\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands_solution.HAND_CONNECTIONS)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hand_detection():\n",
    "    \"\"\"\n",
    "    Main function to perform hand detection with webcam input.\n",
    "    \"\"\"\n",
    "    # webcam\n",
    "    cap = initialize_webcam()\n",
    "    if cap is None:\n",
    "        return\n",
    "\n",
    "    # mediapipe hands\n",
    "    while cap.isOpened():\n",
    "        frame = capture_video(cap)\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        frame_with_landmarks = hand_detection(frame, hands_instance)\n",
    "\n",
    "        display_frame(frame_with_landmarks)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    release_resources(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_capture_only():\n",
    "    \"\"\"\n",
    "    Main function to capture and display webcam frames without hand detection.\n",
    "    \"\"\"\n",
    "    cap = initialize_webcam()\n",
    "    if cap is None:\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame = capture_video(cap)\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        display_frame(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    release_resources(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kalman Filter for hand position\n",
    "def initialize_kalman_filter():\n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "    kf.x = np.array([0., 0., 0., 0.])  # Initial state (x, y, x_velocity, y_velocity)\n",
    "    kf.F = np.array([[1., 0., 1., 0.],\n",
    "                     [0., 1., 0., 1.],\n",
    "                     [0., 0., 1., 0.],\n",
    "                     [0., 0., 0., 1.]])  # State transition matrix\n",
    "    kf.H = np.array([[1., 0., 0., 0.],\n",
    "                     [0., 1., 0., 0.]])  # Measurement matrix\n",
    "    kf.P *= 1000.  # Initial covariance matrix\n",
    "    kf.R = np.array([[5., 0.],\n",
    "                     [0., 5.]])  # Measurement noise covariance\n",
    "    return kf\n",
    "\n",
    "# Initialize two Kalman filters for two hands\n",
    "kalman_filters = [initialize_kalman_filter(), initialize_kalman_filter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kalman_filter(kf, hand_landmarks):\n",
    "    \"\"\"\n",
    "    Apply Kalman Filter to hand landmarks for position smoothing.\n",
    "    \"\"\"\n",
    "    if hand_landmarks:\n",
    "        # Extract x, y coordinates of wrist (landmark 0)\n",
    "        wrist_x = hand_landmarks.landmark[0].x\n",
    "        wrist_y = hand_landmarks.landmark[0].y\n",
    "\n",
    "        # Update Kalman Filter\n",
    "        z = np.array([wrist_x, wrist_y])\n",
    "        kf.predict()\n",
    "        kf.update(z)\n",
    "\n",
    "        # Get the filtered position\n",
    "        filtered_x, filtered_y = kf.x[0], kf.x[1]\n",
    "        return filtered_x, filtered_y\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_detection_with_kalman(frame, hands):\n",
    "    \"\"\"\n",
    "    Perform hand detection using MediaPipe, apply Kalman filter, and recognize gestures.\n",
    "    \"\"\"\n",
    "    # Convert the image to RGB - mediapipe expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process frame\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw landmarks and apply Kalman filter\n",
    "    if results.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Apply the corresponding Kalman filter to each hand\n",
    "            if idx < 2:  # Only handle up to 2 hands\n",
    "                filtered_x, filtered_y = apply_kalman_filter(kalman_filters[idx], hand_landmarks)\n",
    "                \n",
    "                if filtered_x is not None and filtered_y is not None:\n",
    "                    # Draw the filtered position as a circle on the frame\n",
    "                    h, w, _ = frame.shape\n",
    "                    cv2.circle(frame, (int(filtered_x * w), int(filtered_y * h)), 10, (0, 255, 0), -1)\n",
    "\n",
    "            # Draw the original hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands_solution.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Gesture recognition part\n",
    "            predicted_gesture = gesture_recognition_integration(hand_landmarks)\n",
    "            if predicted_gesture is not None:\n",
    "                print(f\"Predicted gesture: {predicted_gesture}\")\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hand_detection_optimized():\n",
    "    \"\"\"\n",
    "    Main function with optimized hand tracking using Kalman filter.\n",
    "    \"\"\"\n",
    "    cap = initialize_webcam()\n",
    "    if cap is None:\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame = capture_video(cap)\n",
    "        if frame is None:\n",
    "            break\n",
    "        \n",
    "        frame_with_landmarks = hand_detection_with_kalman(frame, hands_instance)\n",
    "        display_frame(frame_with_landmarks)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    release_resources(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n",
      "Predicted gesture: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#main_hand_detection()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain_hand_detection_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 14\u001b[0m, in \u001b[0;36mmain_hand_detection_optimized\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m frame_with_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mhand_detection_with_kalman\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhands_instance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m display_frame(frame_with_landmarks)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[56], line 9\u001b[0m, in \u001b[0;36mhand_detection_with_kalman\u001b[1;34m(frame, hands)\u001b[0m\n\u001b[0;32m      6\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Process frame\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Draw landmarks and apply Kalman filter\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\gesture_env\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adria\\anaconda3\\envs\\gesture_env\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main_hand_detection()\n",
    "main_hand_detection_optimized()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
